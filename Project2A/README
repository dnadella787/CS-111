NAME: Dhanush Nadella
UID: 205150583
EMAIL: dnadella787@gmail.com

Included Files:

all relavent research is included in each file it was used in 

lab2_add.c -> c source file that uses concurrency and sync mechanisms to add and subtract 1 as per spec
lab2_list.c -> c source file that uses concurrency to perform operations on sorted doubly linked list
SortedList.h -> header file that provides interface/spec for necessary SortedList operatins
SortedList.c -> implemented SortedList functions 
lab2_add-#.png -> all lab2_add associated graphs created through given script as per spec 
lab2_list-#.png -> all lab2_list associated graphs created through given script as per spec
lab2_add.gp -> given script to produce graphs 
lab2_list.gp -> given script to produce graphs 
Makefile -> supports build, dist, clean, graphs, and tests target as per spec 
README -> this file, includes answers to questions 
test.sh -> bash script to help run all of the tests using for loops, creates the CSV files for the graphs 


Question 2.1.1 - Causing Conflicts:

First, I ran my lab2_add program with 1-15 threads, each variant
with 100, 1000, 10000, and 100000 iterations. I noticed that in 
general, for all of the thread counts (except 1), there was a 
result of failure for 10000 and 100000 iterations consistently

It takes so many iterations to result in failure because as you 
increase the number of iterations, the average time per operation
is consistently decreasing for every thread count as the number
of iterations increases. So as you increase the number of iterations,
there is more context switching, and without any protections, 
it will create more chances that the program is interrupted at 
a point that causes concurrency issues amongst the threads,
i.e. a race condition has a higher chance to occur. 

When the number of iterations is smaller though, the average 
runtime is much higher meaning there is a better chance of 
either the thread running to completion, or just altogether a
lower chance of the thread going through a context switch at 
a bad time because there are less of them happening. 


Question 2.1.2

The --yield runs so much slower because a thread is forced to 
yield each and every single time the add function is called
inside of the helper start_routine() function which is 
(2 x (number of iterations)). This greatly increases the 
number of context switches that occur which in turn increases
the overhead associated with that. This would include saving
registers, scheduling, OS data structures for memory managing,
etc. 

So the additional time would be going towards the context
switching and the associated overhead. 

It is not really possible to get valid per-operation-timings
because the --yield option's per-operation-timings include
the time it takes to do a context switch, that is why it is 
so high. But, if we were to obtain an average estimate of how 
long the context switch takes, we could find a more accurate
per-operation-timings for the --yield option. This would be 
hard because the time it takes could be changing variably
during each threads lifetime. 


Question 2.1.3 

The average time per operation is calculate by simply taking the total
time (which is calculate by taking when we first create threads to 
when they all terminate successfully) and dividing by the amount of 
operations. As the number of iterations increases, the total time has
a smaller and smaller percentage of it coming from the context switches
and more of it coming from simply doing the actual adding/subtracting.
Essentially, the time spent working increases faster than the time
spent on context switches as the iterations increase. 

We can find the "correct" cost by simply running a higher and higher 
amount of iterations. We can define the correct cost as 

                Correct Cost = lim (avg time per op)
                               iterations --> oo 

We cannot realistically run infinity iterations so we simply run a 
very large number to find a cost that is very close to the correct cost. 
So we simply know to run a very very large amount of iterations. 


Question 2.1.4

All options perform similarly for low number of threads because 
there are lower chances that a thread will try to enter the critical
section while another thread is also on the critical section. Thus,
overhead/checks/more code related to keeping a portion of the code 
mutually exclusive are reduced and so are the chances of a race
condition ocurring and producing non deterministic output for the 
add-none tests.

The protected operations slow down as the number of threads increase
because the amount of overhead/checks/code related to making sure
that another thread cannot execute the critical section of code
while another thread holds the lock will continually increase. 
Also more time is waisted because as the number of threads increases,
the number of threads that can hold a lock does not, so only the 
number of threads that need to wait increases everytime. 


Question 2.2.1

As the number of threads increases, the per mutex protected operation
time increases faster for part 1 (adds) than in part 2 (sorted lists).
While the curve in part 1 (adds) grows constantly (linearly), the curve
in part 2 (sorted lists) grows but then plateaus off and grows slower.
This is likely due to the fact that the actual code in part 2 is simply
more tedious in nature as it is working with a data structure rather 
than simply adding and subtracting 1. So as you increase the number of
threads the amount of work on mutual exclusivity is dominated by the 
actual work the threads are doing. Thus the curve for part 2 flattens 
out as the number of threads increases while the curve continues growing
for part 1.


Question 2.2.2

For both mutex and spin locks, the time per protected operation increases
as you increase the number of threads, but it seems that mutex consistently
takes more time per operation than spin locks. In addition mutex moves towards
a plateau while spin lock continues to grow. 

The time per protected operation increases as you increase the number of threads
because as the number of threads increases, the number of threads that cannot 
access a particular piece of code also increases. So there is more work that must
be put into keep out mutual exclusion criterion working. 

The mutex has a higher time per protection operation than the spin
lock which is rather surprising. I would expect that spin locks 
would take a higher amount because they simply burn off CPU cycles.
I assume this is because the tests I ran were low amount of iterations
for the spin blocks to prevent too much CPU usage. I think for a low
amount of threads and low amount of iterations, burning off a few cpu 
cycles is not so bad and is shorter than the mutex alternative so the 
graph indicates that the cost is lower than that of mutex.


